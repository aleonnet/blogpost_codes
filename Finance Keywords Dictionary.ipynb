{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Finance Keywords Dictionary using Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrespective of the Business domain you work in, The presence of Text Data is inevitable. You could be in Audit Team and there are Audit documents or You could be in Entertainment Business and your fans could be tweeting about the Movie or Series that you are producing so Text Data is almost there in every business domain. \n",
    "\n",
    "For any analyst, a large amount of data is nothing less than a scoop of tastier ice cream. But unfortunately, with the huge amount of text data, there comes a huge problem that it's not structured data. To keep matters simple unlike your numeric Table, Text isn't structured or fit into a tabular format where you have got a lot of out-of-box analytics solutions to built upon. \n",
    "\n",
    "Thus, Analysing Text data becomes a unique challenge for an Analyst and in that challenge of trying to make sense of Text data, there is one handy utility required that is \"Keywords Dictionary\". \n",
    "\n",
    "### What is a Keywords Dictionary?\n",
    "\n",
    "A Keywords Dictonary is a set of words put together based on a common theme. Let's take case, that you are a Bank and you want to improve your customer service. For you to make sense of what your customers complaint or talk about, you need to have list of keywords that are related to business and that is where the need to have a Keywords Dictionary comes in. \n",
    "\n",
    "Since Keywords Dictionaries are specific to the business purpose, it might not be available for your request easily unless somoene has built it and open-sourced. Hence, it's always better to build your own custom Keywords Dictionary. \n",
    "\n",
    "#### How to build your own Keywords Dictionary? \n",
    "\n",
    "* First of all, we need to define on a Data Source - which is usually a website with the list of Keywords (that we are interested in). \n",
    "\n",
    "* Extract Website content from the given url\n",
    "\n",
    "* Scrape the desired content (Keywords) from the website content \n",
    "\n",
    "* Clean the scraped data if requried and store locally for future use\n",
    "\n",
    "\n",
    "Now, you could be wondering about a new jargon in one of the points - \"scrape\". It comes from the process called \"Web Scraping\" which \"is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis\" as defined by Wikipedia. \n",
    "\n",
    "#### Getting started with Web Scraping:\n",
    "\n",
    "Python has two wonderful packages for web scraping. 1. BeautifulSoup and 2. Scrapy. We will use \"BeautifulSoup\" in this post to scrape data from the Web. While BeautifulSoup can do the job of parsing the html and making sense of the web content, we need to \"get\" the website in the first place and we will use \"requests\" package for that.  \n",
    "\n",
    "#### How to install requests & BeautifulSoup:\n",
    "\n",
    "Requests can be installed using pip. \n",
    "\n",
    "`pip install requests` - if you are using Python version lesser than 3\n",
    "`pip3 install requests` - if you are using Python version greater than 3\n",
    "\n",
    "\n",
    "BeautifulSoup also can be installed using pip or pip3 if you are using Python 3.x. \n",
    "\n",
    "`pip install beautifulsoup4` \n",
    "`pip3 install beautifulsoup4`\n",
    "\n",
    "Loading both the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "\n",
    "we will use Moneycontrol.com's Glossary page to build our Finance Keywords Dictionary. Note that this post is just for educational purpose and make sure you don't violate the Terms of Service of the websites from which you are trying to scrape. \n",
    "\n",
    "`url = \"http://www.moneycontrol.com/glossary/\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.moneycontrol.com/glossary/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have defined the url, now let us extract the content of the url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = requests.get(url) #sends a GET http request to collec the content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the request was successful by checking the response status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.status_code #200 is succssful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = content.text #extracting the response content as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as the content is ready as text. We can use BeautifulSoup to make a \"soup\" - ideally, parsing the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdrs/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/abdrs/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the above screenshot, what we are interested in the extracted content is the html tag \"a\". But there are so many links in the website that also could include junk like social media links and other irrelevant links. Giving a deep look in the above screenshot could also reveal that our desired urls have a common pattern that is \"/glossary/\". Hence we would be extracting the content with two conditions:\n",
    "\n",
    "* only \"a\" tag\n",
    "* \"a\" tag with \"href\" containing the string \"glossary\" in it \n",
    "\n",
    "To extract all the \"a\" tag links, we will use the function \"find_all()\" and to find the string \"glossary\" in \"href\", we will use \"regex\" for pattern matching using the python package \"re\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_links = soup.find_all(\"a\", href = re.compile(\"glossary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to extract the Keywords, which are nothing but the text values in each of those links that we extracted and stored in \"all_links\". We will use a \"for\" loop to iterate through each element of \"all_links\" and extract \"text\" value of it and store it in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Early (premature) Withdrawal',\n",
       " 'Early Entry',\n",
       " 'Early Retirement Penalty',\n",
       " 'Earned Income',\n",
       " 'Earned Income Rule',\n",
       " 'Earned premium',\n",
       " 'Earnings before taxes',\n",
       " 'Earnings Estimates',\n",
       " 'Earnings form',\n",
       " 'Earnings multiple approach',\n",
       " 'Earnings per Share (EPS)',\n",
       " 'Earnings stripping',\n",
       " 'Earthquake insurance',\n",
       " 'EBIT',\n",
       " 'EBITDA',\n",
       " 'ECN',\n",
       " 'Econometrics',\n",
       " 'Economic double taxation',\n",
       " 'Economic loss',\n",
       " 'Education IRA']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = [] #empty list to store the keywords\n",
    "\n",
    "for link in all_links:\n",
    "    keywords.append(link.text)\n",
    "    \n",
    "len(keywords)\n",
    "\n",
    "keywords[1000:1020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have successfully built a Finance Keywords Dictionary of length 3165. Please note that some of the keywords might need a little bit cleaning and business domain knowledge to further refine before using in your Machine Learning model. This post could be simply replicated for your own need with a simple change of the source url and a few other tweaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
